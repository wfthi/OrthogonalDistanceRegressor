"""
    A toy model to test the logitic classification routine with Orthogonal
    Distance Regression. The code outputs the classes including uncertain assigments
    
    The data (moons) are generated by scikit-learn
    
    Author : Wing-Fai Thi
    
    Licence : GNU v 3.0
    
    History : 14/3/2018
              1/5/2024
    """

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from ODLinear import *

noise = 0.2
X, y =  make_moons(noise=0.3, random_state=0) # generate toy data
X_err = np.full(X.shape,noise)

h = .02  # step size in the mesh
x_min, x_max = X[:, 0].min()-.2, X[:, 0].max()+.2
y_min, y_max = X[:, 1].min()-.2, X[:, 1].max()+.2
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))

poly = PolynomialFeatures(degree=6,include_bias=False)
poly.fit(X)
Xpoly     = poly.transform(X)
Xpoly_err = poly.transform(X_err)

ind = np.arange(0,y.size,1)
ind_train, ind_test, y_train, y_test = train_test_split(ind, y, test_size=0.3,
random_state=0)

X_train = X[ind_train,:]
X_test  = X[ind_test,:]
Xpoly_train = Xpoly[ind_train,:]
Xpoly_test  = Xpoly[ind_test,:]
Xpoly_err_train = Xpoly_err[ind_train,:]
Xpoly_err_test = Xpoly_err[ind_test,:]

v = poly.transform(np.c_[xx.ravel(), yy.ravel()])

clf = OrthogonalDistanceLogisticRegression(C=50, func='tanh')

clf.fit(Xpoly_train, y_train, X_err=Xpoly_err_train)

print("score (train):",clf.score(Xpoly_train,y_train))
print("score (test) :",clf.score(Xpoly_test,y_test))

pred=clf.predict(Xpoly_test)
proba=clf.predict_proba(Xpoly_test)[:,1]
proba_MC_errors=clf.predict_proba_MC_error(Xpoly_test, Xpoly_err_test)

print("logloss score (test) :", clf.logloss_score(y_test, proba))

max_proba = proba+3.*proba_MC_errors
min_proba = proba-3.*proba_MC_errors
w1 = np.array(np.where(max_proba < 0.5))

w2 = np.array(np.where(min_proba > 0.5))
w12 = np.array(np.where((min_proba < 0.5) & (max_proba > 0.5)))
nw1 = w1.size
nw2 = w2.size
nw12 = w12.size

print("class 1:", nw1," / ",np.count_nonzero(proba < 0.5))
print("class 2:", nw2," / ",np.count_nonzero(proba > 0.5))
print("uncertain:", nw12,"/ 0")


# generate the decision function
Z = clf.predict_proba(v)[:, 1]
Z = Z.reshape(xx.shape)

cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])

# 
ax = plt.subplot(111)
# Plot also the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
                    alpha=0.6,label="train")
# and testing points
ax.scatter(X_test[pred == y_test, 0], X_test[pred == y_test, 1], c=y_test[pred == y_test], cmap=cm_bright,
                   alpha=1.0, marker='*',label="test correct")
ax.scatter(X_test[pred != y_test, 0], X_test[pred != y_test, 1], c=y_test[pred != y_test], cmap=cm_bright,
           alpha=1.0, marker='^',label="test incorrect" )

ax.contourf(xx, yy, Z, cmap=cm, alpha=.3)
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.legend()
ax.set_xticks(())
ax.set_yticks(())
ax.set_xlabel("X1")
ax.set_ylabel("X2")
ax.set_title("Polynomial features of degree 6 and Logistic regression")
plt.savefig("ODLinear_moons_classification.png")
plt.tight_layout()
plt.show()
